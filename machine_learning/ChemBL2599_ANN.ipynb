{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d62d9fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7572, 251)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "combined_data = pd.read_csv(\"../data/ChemBL2599_combined_data.csv.gz\", low_memory=False)\n",
    "combined_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d6b0840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Molecular Weight</th>\n",
       "      <th>AlogP</th>\n",
       "      <th>pIC50_m</th>\n",
       "      <th>converted_smile_0</th>\n",
       "      <th>converted_smile_1</th>\n",
       "      <th>converted_smile_2</th>\n",
       "      <th>converted_smile_3</th>\n",
       "      <th>converted_smile_4</th>\n",
       "      <th>converted_smile_5</th>\n",
       "      <th>converted_smile_6</th>\n",
       "      <th>...</th>\n",
       "      <th>converted_smile_190</th>\n",
       "      <th>converted_smile_191</th>\n",
       "      <th>converted_smile_192</th>\n",
       "      <th>converted_smile_193</th>\n",
       "      <th>converted_smile_194</th>\n",
       "      <th>converted_smile_195</th>\n",
       "      <th>converted_smile_196</th>\n",
       "      <th>converted_smile_197</th>\n",
       "      <th>converted_smile_198</th>\n",
       "      <th>converted_smile_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>369.47</td>\n",
       "      <td>2.52</td>\n",
       "      <td>6.499997</td>\n",
       "      <td>1.854377</td>\n",
       "      <td>808.395217</td>\n",
       "      <td>19.388541</td>\n",
       "      <td>15.774469</td>\n",
       "      <td>15.774469</td>\n",
       "      <td>12.935561</td>\n",
       "      <td>9.373808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.614840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>355.45</td>\n",
       "      <td>2.01</td>\n",
       "      <td>7.500038</td>\n",
       "      <td>1.858084</td>\n",
       "      <td>808.121772</td>\n",
       "      <td>18.681434</td>\n",
       "      <td>15.119768</td>\n",
       "      <td>15.119768</td>\n",
       "      <td>12.435561</td>\n",
       "      <td>8.847099</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.644743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>375.86</td>\n",
       "      <td>2.36</td>\n",
       "      <td>6.799998</td>\n",
       "      <td>1.858084</td>\n",
       "      <td>813.693554</td>\n",
       "      <td>18.681434</td>\n",
       "      <td>14.497733</td>\n",
       "      <td>15.253662</td>\n",
       "      <td>12.435561</td>\n",
       "      <td>8.536081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.630094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>367.46</td>\n",
       "      <td>1.88</td>\n",
       "      <td>6.599998</td>\n",
       "      <td>1.582716</td>\n",
       "      <td>853.946965</td>\n",
       "      <td>18.802754</td>\n",
       "      <td>15.241088</td>\n",
       "      <td>15.241088</td>\n",
       "      <td>13.097357</td>\n",
       "      <td>9.554206</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.634156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>359.39</td>\n",
       "      <td>3.25</td>\n",
       "      <td>6.657577</td>\n",
       "      <td>1.610247</td>\n",
       "      <td>1135.828414</td>\n",
       "      <td>18.802754</td>\n",
       "      <td>14.523392</td>\n",
       "      <td>14.523392</td>\n",
       "      <td>13.097357</td>\n",
       "      <td>8.296359</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.433853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 203 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Molecular Weight  AlogP   pIC50_m  converted_smile_0  converted_smile_1  \\\n",
       "0            369.47   2.52  6.499997           1.854377         808.395217   \n",
       "1            355.45   2.01  7.500038           1.858084         808.121772   \n",
       "2            375.86   2.36  6.799998           1.858084         813.693554   \n",
       "3            367.46   1.88  6.599998           1.582716         853.946965   \n",
       "4            359.39   3.25  6.657577           1.610247        1135.828414   \n",
       "\n",
       "   converted_smile_2  converted_smile_3  converted_smile_4  converted_smile_5  \\\n",
       "0          19.388541          15.774469          15.774469          12.935561   \n",
       "1          18.681434          15.119768          15.119768          12.435561   \n",
       "2          18.681434          14.497733          15.253662          12.435561   \n",
       "3          18.802754          15.241088          15.241088          13.097357   \n",
       "4          18.802754          14.523392          14.523392          13.097357   \n",
       "\n",
       "   converted_smile_6  ...  converted_smile_190  converted_smile_191  \\\n",
       "0           9.373808  ...                  0.0                  0.0   \n",
       "1           8.847099  ...                  0.0                  0.0   \n",
       "2           8.536081  ...                  0.0                  0.0   \n",
       "3           9.554206  ...                  0.0                  0.0   \n",
       "4           8.296359  ...                  0.0                  0.0   \n",
       "\n",
       "   converted_smile_192  converted_smile_193  converted_smile_194  \\\n",
       "0                  0.0                  0.0                  0.0   \n",
       "1                  0.0                  0.0                  0.0   \n",
       "2                  0.0                  0.0                  0.0   \n",
       "3                  0.0                  0.0                  0.0   \n",
       "4                  0.0                  0.0                  0.0   \n",
       "\n",
       "   converted_smile_195  converted_smile_196  converted_smile_197  \\\n",
       "0                  0.0                  0.0                  0.0   \n",
       "1                  0.0                  0.0                  0.0   \n",
       "2                  0.0                  0.0                  0.0   \n",
       "3                  0.0                  0.0                  0.0   \n",
       "4                  0.0                  0.0                  0.0   \n",
       "\n",
       "   converted_smile_198  converted_smile_199  \n",
       "0                  0.0             0.614840  \n",
       "1                  0.0             0.644743  \n",
       "2                  0.0             0.630094  \n",
       "3                  0.0             0.634156  \n",
       "4                  0.0             0.433853  \n",
       "\n",
       "[5 rows x 203 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove non numerical unused attributes\n",
    "machine_learning_df = combined_data.drop(columns=[\"Molecule ChEMBL ID\", \"Molecule Name\", \"Molecule Max Phase\", \"#RO5 Violations\", \n",
    "                                                  \"Compound Key\", \"Smiles\", \"Standard Type\", \"Standard Relation\", \"Standard Value\",\t\"Standard Units\", \n",
    "                                                  \"pChEMBL Value\", \"Data Validity Comment\",\t\"Comment\", \"Uo Units\", \"Ligand Efficiency BEI\",\t\"Ligand Efficiency LE\",\n",
    "                                                    \"Ligand Efficiency LLE\", \"Ligand Efficiency SEI\", \"Potential Duplicate\", \"Assay ChEMBL ID\", \"Assay Description\",\n",
    "                                                    \"Assay Type\", \"BAO Format ID\", \"BAO Label\", \"Assay Organism\", \"Assay Tissue ChEMBL ID\", \"Assay Tissue Name\",\t\n",
    "                                                    \"Assay Cell Type\", \"Assay Subcellular Fraction\", \"Assay Parameters\", \"Assay Variant Accession\", \n",
    "                                                    \"Assay Variant Mutation\", \"Target ChEMBL ID\", \"Target Name\", \"Target Organism\", \"Target Type\", \"Document ChEMBL ID\",\n",
    "                                                    \"Source ID\", \"Source Description\", \"Document Journal\", \"Document Year\", \"Cell ChEMBL ID\", \"Properties\", \"Action Type\",\n",
    "                                                    \"Standard Text Value\", \"Value\", \"IC50_m\", \"Converted Smiles\"])\n",
    "machine_learning_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c056381c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Iterable, Union\n",
    "from sklearn.utils.validation import check_array, column_or_1d\n",
    "from inspect import isclass\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "\n",
    "def check_is_fitted(applicability_domain,\n",
    "                    attributes: Union[str, List[str], Tuple[str]],\n",
    "                    msg: str = None,\n",
    "                    all_or_any=all):\n",
    "\n",
    "    if isclass(applicability_domain):\n",
    "        raise TypeError(\"{} is a class, not an instance.\".format(applicability_domain))\n",
    "    if msg is None:\n",
    "        msg = (\n",
    "            \"This %(name)s instance is not fitted yet. Call 'fit' with \"\n",
    "            \"appropriate arguments before using this applicability domain.\"\n",
    "        )\n",
    "    if not hasattr(applicability_domain, \"fit\"):\n",
    "        raise TypeError(\"%s is not an estimator instance.\" % (applicability_domain))\n",
    "    if not isinstance(attributes, (list, tuple)):\n",
    "        attributes = [attributes]\n",
    "    is_fitted = all_or_any([hasattr(applicability_domain, attr) for attr in attributes])\n",
    "    if not is_fitted:\n",
    "        raise NotFittedError(msg % {\"name\": type(applicability_domain).__name__})\n",
    "\n",
    "class ApplicabilityDomain(ABC):\n",
    "    def __init__(self):\n",
    "        self.fitted_ = False\n",
    "\n",
    "    def fit(self, X):\n",
    "        X = check_array(X)\n",
    "        self.num_points, self.num_dims = X.shape\n",
    "        self._fit(X)\n",
    "        self.fitted_ = True\n",
    "\n",
    "    @abstractmethod\n",
    "    def _fit(self, X):\n",
    "        pass\n",
    "\n",
    "    def contains(self, sample) -> Union[bool, Iterable[bool]]:\n",
    "        check_is_fitted(self, 'fitted_')\n",
    "        try:\n",
    "            sample = column_or_1d(sample)\n",
    "        except ValueError:\n",
    "            sample = check_array(sample, accept_large_sparse=False)\n",
    "        if sample.ndim == 1 and sample.shape[0] != self.num_dims:\n",
    "            raise ValueError('sample must have the same number of features as the applicability domain; '\n",
    "                             f'{sample.shape[0]} and {self.num_dims} respectively')\n",
    "        elif sample.ndim == 2 and sample.shape[1] != self.num_dims:\n",
    "            raise ValueError('sample must have the same number of features as the applicability domain; '\n",
    "                             f'{sample.shape[1]} and {self.num_dims} respectively')\n",
    "        return self._contains(sample)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _contains(self, sample):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3127856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "from typing import Union, Tuple, Optional\n",
    "import numpy as np\n",
    "import scipy\n",
    "from numpy.random import RandomState\n",
    "from scipy.spatial.distance import cdist, _METRICS as dist_fns\n",
    "from scipy.stats import f as Fdistrib\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor, NearestNeighbors\n",
    "from sklearn.neighbors._kde import KernelDensity\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, MaxAbsScaler, StandardScaler\n",
    "from sklearn.utils.extmath import stable_cumsum\n",
    "\n",
    "class TopKatApplicabilityDomain(ApplicabilityDomain):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def _fit(self, X):\n",
    "        self.X_min_, self.X_max_ = X.min(axis=0), X.max(axis=0)\n",
    "        S = (2 * X - self.X_max_ - self.X_min_) / np.where((self.X_max_ - self.X_min_) != 0,\n",
    "                                                           (self.X_max_ - self.X_min_),1)\n",
    "        S = np.c_[np.ones(S.shape[0]), S]\n",
    "        self.eigen_val, self.eigen_vec = np.linalg.eig(S.T.dot(S))\n",
    "        self.eigen_val, self.eigen_vec = np.real(self.eigen_val), np.real(self.eigen_vec)\n",
    "        OPS = S.dot(self.eigen_vec)\n",
    "        self.OPS_min_ = OPS.min(axis=0)\n",
    "        self.OPS_max_ = OPS.max(axis=0)\n",
    "\n",
    "    def _contains(self, sample):\n",
    "        Ssample = (2 * sample - self.X_max_ - self.X_min_) / np.where((self.X_max_ - self.X_min_) != 0,\n",
    "                                                                      (self.X_max_ - self.X_min_),1)\n",
    "        if sample.ndim == 1:\n",
    "            Ssample = np.c_[1, Ssample.reshape((1, -1))]\n",
    "        else:\n",
    "            Ssample = np.c_[np.ones((sample.shape[0], 1)), Ssample]\n",
    "        OPS_sample = Ssample.dot(self.eigen_vec)\n",
    "        denom = np.divide(np.ones_like(self.eigen_val, dtype=float),\n",
    "                          self.eigen_val,\n",
    "                          out=np.zeros_like(self.eigen_val),\n",
    "                          where=self.eigen_val!=0)\n",
    "        dOPS = (OPS_sample * OPS_sample).dot(denom)\n",
    "        if sample.ndim == 1 and isinstance(dOPS, np.ndarray):\n",
    "            dOPS = dOPS.item()\n",
    "        return dOPS < (5 * (self.num_dims)) / (2 * self.num_points)\n",
    "\n",
    "\n",
    "class LeverageApplicabilityDomain(ApplicabilityDomain):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def _fit(self, X):\n",
    "        X = self.scaler.fit_transform(X)\n",
    "        self.var_covar = np.linalg.inv(X.T.dot(X))\n",
    "        self.threshold = 3 * (self.num_dims + 1) / self.num_points\n",
    "\n",
    "    def _contains(self, sample):\n",
    "        if sample.ndim == 1:\n",
    "            sample = self.scaler.transform(sample.reshape(1, -1))\n",
    "            h = sample.dot(self.var_covar).dot(sample.T)\n",
    "        else:\n",
    "            sample = self.scaler.transform(sample)\n",
    "            h = np.diag(sample.dot(self.var_covar).dot(sample.T))\n",
    "        return h < self.threshold\n",
    "\n",
    "class KNNApplicabilityDomain(ApplicabilityDomain):\n",
    "    def __init__(self, k: int = 5,\n",
    "                 alpha: float = 0.95,\n",
    "                 hard_threshold: float = None,\n",
    "                 scaling: Optional[str] = 'robust',\n",
    "                 dist: str = 'euclidean',\n",
    "                 scaler_kwargs=None,\n",
    "                 njobs: int=1):\n",
    "        super().__init__()\n",
    "        if scaler_kwargs is None:\n",
    "            scaler_kwargs = {}\n",
    "        if alpha > 1 or alpha < 0:\n",
    "            raise ValueError('alpha must lie between 0 and 1')\n",
    "        scaling_methods = ('robust', 'minmax', 'maxabs', 'standard', None)\n",
    "        if scaling not in scaling_methods:\n",
    "            raise ValueError(f'scaling method must be one of {scaling_methods}')\n",
    "        if scaling == 'robust':\n",
    "            self.scaler = RobustScaler(**scaler_kwargs)\n",
    "        elif scaling == 'minmax':\n",
    "            self.scaler = MinMaxScaler(**scaler_kwargs)\n",
    "        elif scaling == 'maxabs':\n",
    "            self.scaler = MaxAbsScaler(**scaler_kwargs)\n",
    "        elif scaling == 'standard':\n",
    "            self.scaler = StandardScaler(**scaler_kwargs)\n",
    "        elif scaling is None:\n",
    "            self.scaler = None\n",
    "        else:\n",
    "            raise NotImplementedError('scaling method not implemented')\n",
    "        if dist not in dist_fns.keys():\n",
    "            raise NotImplementedError('distance type is not available')\n",
    "        else:\n",
    "            self.dist = dist\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.hard_threshold = hard_threshold\n",
    "        self.nn = NearestNeighbors(n_neighbors=k, metric=dist, n_jobs=njobs)\n",
    "\n",
    "    def _fit(self, X):\n",
    "        self.X_norm = self.scaler.fit_transform(X) if self.scaler is not None else X\n",
    "        self.nn.fit(self.X_norm)\n",
    "        self.kNN_dist = self.nn.kneighbors(self.X_norm, return_distance=True, n_neighbors=self.k+1)[0][:, 1:].mean(axis=1)\n",
    "        kNN_train_distance_sorted_ = np.trim_zeros(np.sort(self.kNN_dist))\n",
    "        if self.hard_threshold:\n",
    "            self.threshold_ = self.hard_threshold\n",
    "        else:\n",
    "            self.threshold_ = kNN_train_distance_sorted_[floor(kNN_train_distance_sorted_.shape[0] * self.alpha) - 1]\n",
    "        return self\n",
    "\n",
    "    def _contains(self, sample):\n",
    "        if self.scaler is not None:\n",
    "            if sample.ndim == 1:\n",
    "                sample = self.scaler.transform(sample.reshape((1, len(sample))))\n",
    "            else:\n",
    "                sample = self.scaler.transform(sample)\n",
    "        kNN_sample_dist = self.nn.kneighbors(sample, return_distance=True)[0].mean(axis=1)\n",
    "        norm_dist = kNN_sample_dist / self.threshold_\n",
    "        if self.hard_threshold:\n",
    "            return norm_dist < 1\n",
    "        return norm_dist <= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ed823df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 60 Complete [00h 00m 23s]\n",
      "val_mse: 0.5946145057678223\n",
      "\n",
      "Best val_mse So Far: 0.39134538173675537\n",
      "Total elapsed time: 00h 24m 29s\n",
      "\u001b[1m 48/190\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rafael\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 18 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Training R2: 0.7759053814214373, Test R2: 0.6533567308379751\n",
      "Training MSE: 0.3001754105209001, Test MSE: 0.4511032056307784\n",
      "Training MAE: 0.4020461176081125, Test MAE: 0.487991071521595\n",
      "Best hyperparameters:\n",
      "neurons1: 256\n",
      "neurons2: 128\n",
      "neurons3: 64\n",
      "learning_rate: 0.0001\n",
      "dropout_rate: 0.0\n"
     ]
    }
   ],
   "source": [
    "import keras_tuner as kt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from tensorflow import keras\n",
    "\n",
    "X = machine_learning_df.drop(columns=[\"pIC50_m\"]).values\n",
    "y = machine_learning_df[\"pIC50_m\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_valid, X_train_sub = X_train[:500], X_train[500:]\n",
    "y_valid, y_train_sub = y_train[:500], y_train[500:]\n",
    "\n",
    "def create_model(neurons1, neurons2, neurons3, lr, dropout_rate):\n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Dense(neurons1, activation=\"relu\", input_shape=(X_train.shape[1],)),\n",
    "        keras.layers.Dropout(dropout_rate),\n",
    "        keras.layers.Dense(neurons2, activation=\"relu\"),\n",
    "        keras.layers.Dropout(dropout_rate),\n",
    "        keras.layers.Dense(neurons3, activation=\"relu\"),\n",
    "        keras.layers.Dropout(dropout_rate),\n",
    "        keras.layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        loss=\"mse\",\n",
    "        metrics=[\"mse\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def build_model(hp):\n",
    "    neurons1 = hp.Int(\"neurons1\", min_value=32, max_value=256, step=32)\n",
    "    neurons2 = hp.Int(\"neurons2\", min_value=16, max_value=128, step=16)\n",
    "    neurons3 = hp.Int(\"neurons3\", min_value=8, max_value=64, step=8)\n",
    "    lr = hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "    dropout_rate = hp.Float(\"dropout_rate\", min_value=0.0, max_value=0.5, step=0.05)\n",
    "    return create_model(neurons1, neurons2, neurons3, lr, dropout_rate)\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_model,\n",
    "    objective='val_mse',\n",
    "    max_trials=60,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_mse',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "tuner.search(\n",
    "    X_train_sub, y_train_sub,\n",
    "    epochs=100,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "y_pred_train = best_model.predict(X_train)\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Training R2: {r2_train}, Test R2: {r2_test}\")\n",
    "print(f\"Training MSE: {mse_train}, Test MSE: {mse_test}\")\n",
    "print(f\"Training MAE: {mae_train}, Test MAE: {mae_test}\")\n",
    "\n",
    "print(\"Best hyperparameters:\")\n",
    "print(f\"neurons1: {best_hps.get('neurons1')}\")\n",
    "print(f\"neurons2: {best_hps.get('neurons2')}\")\n",
    "print(f\"neurons3: {best_hps.get('neurons3')}\")\n",
    "print(f\"learning_rate: {best_hps.get('learning_rate')}\")\n",
    "print(f\"dropout_rate: {best_hps.get('dropout_rate')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84a1ec95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rafael\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 32.6124 - mse: 32.6124 - val_loss: 1.9872 - val_mse: 1.9872\n",
      "Epoch 2/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.6799 - mse: 1.6799 - val_loss: 1.1818 - val_mse: 1.1818\n",
      "Epoch 3/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9911 - mse: 0.9911 - val_loss: 1.0854 - val_mse: 1.0854\n",
      "Epoch 4/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9617 - mse: 0.9617 - val_loss: 1.0208 - val_mse: 1.0208\n",
      "Epoch 5/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9113 - mse: 0.9113 - val_loss: 0.9828 - val_mse: 0.9828\n",
      "Epoch 6/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8448 - mse: 0.8448 - val_loss: 0.9650 - val_mse: 0.9650\n",
      "Epoch 7/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8608 - mse: 0.8608 - val_loss: 0.9393 - val_mse: 0.9393\n",
      "Epoch 8/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.8155 - mse: 0.8155 - val_loss: 0.9209 - val_mse: 0.9209\n",
      "Epoch 9/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7876 - mse: 0.7876 - val_loss: 0.8940 - val_mse: 0.8940\n",
      "Epoch 10/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7569 - mse: 0.7569 - val_loss: 0.8788 - val_mse: 0.8788\n",
      "Epoch 11/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.7889 - mse: 0.7889 - val_loss: 0.8571 - val_mse: 0.8571\n",
      "Epoch 12/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.7454 - mse: 0.7454 - val_loss: 0.8503 - val_mse: 0.8503\n",
      "Epoch 13/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.7232 - mse: 0.7232 - val_loss: 0.8868 - val_mse: 0.8868\n",
      "Epoch 14/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.6987 - mse: 0.6987 - val_loss: 0.8202 - val_mse: 0.8202\n",
      "Epoch 15/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.7001 - mse: 0.7001 - val_loss: 0.7819 - val_mse: 0.7819\n",
      "Epoch 16/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.6833 - mse: 0.6833 - val_loss: 0.7693 - val_mse: 0.7693\n",
      "Epoch 17/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.6426 - mse: 0.6426 - val_loss: 0.7392 - val_mse: 0.7392\n",
      "Epoch 18/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.6176 - mse: 0.6176 - val_loss: 0.7336 - val_mse: 0.7336\n",
      "Epoch 19/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.6373 - mse: 0.6373 - val_loss: 0.7591 - val_mse: 0.7591\n",
      "Epoch 20/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.6175 - mse: 0.6175 - val_loss: 0.6939 - val_mse: 0.6939\n",
      "Epoch 21/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.5929 - mse: 0.5929 - val_loss: 0.6881 - val_mse: 0.6881\n",
      "Epoch 22/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5929 - mse: 0.5929 - val_loss: 0.6692 - val_mse: 0.6692\n",
      "Epoch 23/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5597 - mse: 0.5597 - val_loss: 0.6549 - val_mse: 0.6549\n",
      "Epoch 24/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5468 - mse: 0.5468 - val_loss: 0.6773 - val_mse: 0.6773\n",
      "Epoch 25/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5443 - mse: 0.5443 - val_loss: 0.6309 - val_mse: 0.6309\n",
      "Epoch 26/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5521 - mse: 0.5521 - val_loss: 0.6199 - val_mse: 0.6199\n",
      "Epoch 27/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.5157 - mse: 0.5157 - val_loss: 0.6215 - val_mse: 0.6215\n",
      "Epoch 28/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4873 - mse: 0.4873 - val_loss: 0.6296 - val_mse: 0.6296\n",
      "Epoch 29/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4808 - mse: 0.4808 - val_loss: 0.6037 - val_mse: 0.6037\n",
      "Epoch 30/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4993 - mse: 0.4993 - val_loss: 0.5986 - val_mse: 0.5986\n",
      "Epoch 31/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4791 - mse: 0.4791 - val_loss: 0.5848 - val_mse: 0.5848\n",
      "Epoch 32/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4757 - mse: 0.4757 - val_loss: 0.6018 - val_mse: 0.6018\n",
      "Epoch 33/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4783 - mse: 0.4783 - val_loss: 0.6087 - val_mse: 0.6087\n",
      "Epoch 34/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4502 - mse: 0.4502 - val_loss: 0.5726 - val_mse: 0.5726\n",
      "Epoch 35/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4188 - mse: 0.4188 - val_loss: 0.5669 - val_mse: 0.5669\n",
      "Epoch 36/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.4298 - mse: 0.4298 - val_loss: 0.5701 - val_mse: 0.5701\n",
      "Epoch 37/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.4274 - mse: 0.4274 - val_loss: 0.5538 - val_mse: 0.5538\n",
      "Epoch 38/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.4310 - mse: 0.4310 - val_loss: 0.5711 - val_mse: 0.5711\n",
      "Epoch 39/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.4151 - mse: 0.4151 - val_loss: 0.5533 - val_mse: 0.5533\n",
      "Epoch 40/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.4076 - mse: 0.4076 - val_loss: 0.5429 - val_mse: 0.5429\n",
      "Epoch 41/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.3916 - mse: 0.3916 - val_loss: 0.5517 - val_mse: 0.5517\n",
      "Epoch 42/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.3924 - mse: 0.3924 - val_loss: 0.5529 - val_mse: 0.5529\n",
      "Epoch 43/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.3722 - mse: 0.3722 - val_loss: 0.5373 - val_mse: 0.5373\n",
      "Epoch 44/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3947 - mse: 0.3947 - val_loss: 0.5350 - val_mse: 0.5350\n",
      "Epoch 45/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.3744 - mse: 0.3744 - val_loss: 0.5233 - val_mse: 0.5233\n",
      "Epoch 46/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 0.3747 - mse: 0.3747 - val_loss: 0.5194 - val_mse: 0.5194\n",
      "Epoch 47/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.3655 - mse: 0.3655 - val_loss: 0.5250 - val_mse: 0.5250\n",
      "Epoch 48/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.3847 - mse: 0.3847 - val_loss: 0.5342 - val_mse: 0.5342\n",
      "Epoch 49/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 0.3735 - mse: 0.3735 - val_loss: 0.5186 - val_mse: 0.5186\n",
      "Epoch 50/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.3681 - mse: 0.3681 - val_loss: 0.5201 - val_mse: 0.5201\n",
      "Epoch 51/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3683 - mse: 0.3683 - val_loss: 0.5160 - val_mse: 0.5160\n",
      "Epoch 52/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.3590 - mse: 0.3590 - val_loss: 0.5178 - val_mse: 0.5178\n",
      "Epoch 53/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.3439 - mse: 0.3439 - val_loss: 0.5335 - val_mse: 0.5335\n",
      "Epoch 54/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3540 - mse: 0.3540 - val_loss: 0.5226 - val_mse: 0.5226\n",
      "Epoch 55/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3250 - mse: 0.3250 - val_loss: 0.5077 - val_mse: 0.5077\n",
      "Epoch 56/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3290 - mse: 0.3290 - val_loss: 0.5133 - val_mse: 0.5133\n",
      "Epoch 57/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3081 - mse: 0.3081 - val_loss: 0.5057 - val_mse: 0.5057\n",
      "Epoch 58/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3269 - mse: 0.3269 - val_loss: 0.5151 - val_mse: 0.5151\n",
      "Epoch 59/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3238 - mse: 0.3238 - val_loss: 0.5285 - val_mse: 0.5285\n",
      "Epoch 60/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3412 - mse: 0.3412 - val_loss: 0.5026 - val_mse: 0.5026\n",
      "Epoch 61/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3159 - mse: 0.3159 - val_loss: 0.5137 - val_mse: 0.5137\n",
      "Epoch 62/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3129 - mse: 0.3129 - val_loss: 0.5207 - val_mse: 0.5207\n",
      "Epoch 63/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2915 - mse: 0.2915 - val_loss: 0.5027 - val_mse: 0.5027\n",
      "Epoch 64/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3139 - mse: 0.3139 - val_loss: 0.5005 - val_mse: 0.5005\n",
      "Epoch 65/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3100 - mse: 0.3100 - val_loss: 0.4999 - val_mse: 0.4999\n",
      "Epoch 66/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3033 - mse: 0.3033 - val_loss: 0.4937 - val_mse: 0.4937\n",
      "Epoch 67/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2781 - mse: 0.2781 - val_loss: 0.4913 - val_mse: 0.4913\n",
      "Epoch 68/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2957 - mse: 0.2957 - val_loss: 0.4993 - val_mse: 0.4993\n",
      "Epoch 69/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2933 - mse: 0.2933 - val_loss: 0.4923 - val_mse: 0.4923\n",
      "Epoch 70/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2919 - mse: 0.2919 - val_loss: 0.4948 - val_mse: 0.4948\n",
      "Epoch 71/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2759 - mse: 0.2759 - val_loss: 0.5141 - val_mse: 0.5141\n",
      "Epoch 72/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2729 - mse: 0.2729 - val_loss: 0.5002 - val_mse: 0.5002\n",
      "Epoch 73/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2822 - mse: 0.2822 - val_loss: 0.4902 - val_mse: 0.4902\n",
      "Epoch 74/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2682 - mse: 0.2682 - val_loss: 0.5498 - val_mse: 0.5498\n",
      "Epoch 75/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2804 - mse: 0.2804 - val_loss: 0.4987 - val_mse: 0.4987\n",
      "Epoch 76/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2634 - mse: 0.2634 - val_loss: 0.4981 - val_mse: 0.4981\n",
      "Epoch 77/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2856 - mse: 0.2856 - val_loss: 0.5013 - val_mse: 0.5013\n",
      "Epoch 78/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2671 - mse: 0.2671 - val_loss: 0.4912 - val_mse: 0.4912\n",
      "Epoch 79/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2396 - mse: 0.2396 - val_loss: 0.4915 - val_mse: 0.4915\n",
      "Epoch 80/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2530 - mse: 0.2530 - val_loss: 0.5132 - val_mse: 0.5132\n",
      "Epoch 81/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2589 - mse: 0.2589 - val_loss: 0.4973 - val_mse: 0.4973\n",
      "Epoch 82/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2520 - mse: 0.2520 - val_loss: 0.5178 - val_mse: 0.5178\n",
      "Epoch 83/100\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2666 - mse: 0.2666 - val_loss: 0.5292 - val_mse: 0.5292\n",
      "\u001b[1m190/190\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 656us/step\n",
      "Training R2: 0.7696814911343481, Test R2: 0.6531208276723892\n",
      "Training MSE: 0.30851233013911505, Test MSE: 0.45141019752613964\n",
      "Training MAE: 0.4025817758856097, Test MAE: 0.4829606809068383\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "neurons1 = 256\n",
    "neurons2 = 128\n",
    "neurons3 = 64\n",
    "learning_rate = 0.0001\n",
    "dropout_rate = 0.0\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(neurons1, activation=\"relu\", input_shape=(X_train.shape[1],)),\n",
    "    keras.layers.Dropout(dropout_rate),\n",
    "    keras.layers.Dense(neurons2, activation=\"relu\"),\n",
    "    keras.layers.Dropout(dropout_rate),\n",
    "    keras.layers.Dense(neurons3, activation=\"relu\"),\n",
    "    keras.layers.Dropout(dropout_rate),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss=\"mse\",\n",
    "    metrics=[\"mse\"]\n",
    ")\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_mse',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "mae_train = mean_absolute_error(y_train, y_pred_train)\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Training R2: {r2_train}, Test R2: {r2_test}\")\n",
    "print(f\"Training MSE: {mse_train}, Test MSE: {mse_test}\")\n",
    "print(f\"Training MAE: {mae_train}, Test MAE: {mae_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5c701ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R2: 0.758136332939381, Test R2: 0.6527584260020327\n",
      "Training MSE: 0.3074352962647974, Test MSE: 0.4274571015772888\n",
      "Training MAE: 0.40378848239698134, Test MAE: 0.47143375447604696\n"
     ]
    }
   ],
   "source": [
    "# The following applicability domains use variables produced from the ANN code above.\n",
    "\n",
    "ad = TopKatApplicabilityDomain()\n",
    "ad.fit(X_train)\n",
    "\n",
    "inside_ad_train = ad.contains(X_train)\n",
    "inside_ad_test = ad.contains(X_test)\n",
    "\n",
    "y_train_inside = y_train[inside_ad_train]\n",
    "y_pred_train_inside = y_pred_train[inside_ad_train]\n",
    "\n",
    "y_test_inside = y_test[inside_ad_test]\n",
    "y_pred_test_inside = y_pred_test[inside_ad_test]\n",
    "\n",
    "r2_train = r2_score(y_train_inside, y_pred_train_inside)\n",
    "r2_test = r2_score(y_test_inside, y_pred_test_inside)\n",
    "mse_train = mean_squared_error(y_train_inside, y_pred_train_inside)\n",
    "mse_test = mean_squared_error(y_test_inside, y_pred_test_inside)\n",
    "mae_train = mean_absolute_error(y_train_inside, y_pred_train_inside)\n",
    "mae_test = mean_absolute_error(y_test_inside, y_pred_test_inside)\n",
    "\n",
    "print(f\"Training R2: {r2_train}, Test R2: {r2_test}\")\n",
    "print(f\"Training MSE: {mse_train}, Test MSE: {mse_test}\")\n",
    "print(f\"Training MAE: {mae_train}, Test MAE: {mae_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e47f359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R2: 0.7716420827701251, Test R2: 0.6563263569442341\n",
      "Training MSE: 0.311985795988654, Test MSE: 0.4616764034674082\n",
      "Training MAE: 0.4047105621657717, Test MAE: 0.48835161900927826\n"
     ]
    }
   ],
   "source": [
    "# The following applicability domains use variables produced from the ANN code above.\n",
    "\n",
    "ad = KNNApplicabilityDomain()\n",
    "ad.fit(X_train)\n",
    "\n",
    "inside_ad_train = ad.contains(X_train)\n",
    "inside_ad_test = ad.contains(X_test)\n",
    "\n",
    "y_train_inside = y_train[inside_ad_train]\n",
    "y_pred_train_inside = y_pred_train[inside_ad_train]\n",
    "\n",
    "y_test_inside = y_test[inside_ad_test]\n",
    "y_pred_test_inside = y_pred_test[inside_ad_test]\n",
    "\n",
    "r2_train = r2_score(y_train_inside, y_pred_train_inside)\n",
    "r2_test = r2_score(y_test_inside, y_pred_test_inside)\n",
    "mse_train = mean_squared_error(y_train_inside, y_pred_train_inside)\n",
    "mse_test = mean_squared_error(y_test_inside, y_pred_test_inside)\n",
    "mae_train = mean_absolute_error(y_train_inside, y_pred_train_inside)\n",
    "mae_test = mean_absolute_error(y_test_inside, y_pred_test_inside)\n",
    "\n",
    "print(f\"Training R2: {r2_train}, Test R2: {r2_test}\")\n",
    "print(f\"Training MSE: {mse_train}, Test MSE: {mse_test}\")\n",
    "print(f\"Training MAE: {mae_train}, Test MAE: {mae_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "def5646d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 74 outliers from training set.\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rafael\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 29.9860 - mse: 29.9860 - val_loss: 1.8774 - val_mse: 1.8774\n",
      "Epoch 2/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.4540 - mse: 1.4540 - val_loss: 1.0691 - val_mse: 1.0691\n",
      "Epoch 3/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9360 - mse: 0.9360 - val_loss: 0.9906 - val_mse: 0.9906\n",
      "Epoch 4/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8956 - mse: 0.8956 - val_loss: 0.9496 - val_mse: 0.9496\n",
      "Epoch 5/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8183 - mse: 0.8183 - val_loss: 0.9136 - val_mse: 0.9136\n",
      "Epoch 6/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8320 - mse: 0.8320 - val_loss: 0.8998 - val_mse: 0.8998\n",
      "Epoch 7/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7924 - mse: 0.7924 - val_loss: 0.8709 - val_mse: 0.8709\n",
      "Epoch 8/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7913 - mse: 0.7913 - val_loss: 0.8522 - val_mse: 0.8522\n",
      "Epoch 9/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7493 - mse: 0.7493 - val_loss: 0.8349 - val_mse: 0.8349\n",
      "Epoch 10/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7064 - mse: 0.7064 - val_loss: 0.8204 - val_mse: 0.8204\n",
      "Epoch 11/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7086 - mse: 0.7086 - val_loss: 0.8219 - val_mse: 0.8219\n",
      "Epoch 12/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7192 - mse: 0.7192 - val_loss: 0.8042 - val_mse: 0.8042\n",
      "Epoch 13/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6644 - mse: 0.6644 - val_loss: 0.7689 - val_mse: 0.7689\n",
      "Epoch 14/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6864 - mse: 0.6864 - val_loss: 0.8124 - val_mse: 0.8124\n",
      "Epoch 15/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6533 - mse: 0.6533 - val_loss: 0.7521 - val_mse: 0.7521\n",
      "Epoch 16/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6661 - mse: 0.6661 - val_loss: 0.7266 - val_mse: 0.7266\n",
      "Epoch 17/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6505 - mse: 0.6505 - val_loss: 0.7173 - val_mse: 0.7173\n",
      "Epoch 18/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6154 - mse: 0.6154 - val_loss: 0.7044 - val_mse: 0.7044\n",
      "Epoch 19/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5906 - mse: 0.5906 - val_loss: 0.7011 - val_mse: 0.7011\n",
      "Epoch 20/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6139 - mse: 0.6139 - val_loss: 0.6752 - val_mse: 0.6752\n",
      "Epoch 21/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5788 - mse: 0.5788 - val_loss: 0.6702 - val_mse: 0.6702\n",
      "Epoch 22/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5701 - mse: 0.5701 - val_loss: 0.6781 - val_mse: 0.6781\n",
      "Epoch 23/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5469 - mse: 0.5469 - val_loss: 0.6776 - val_mse: 0.6776\n",
      "Epoch 24/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5384 - mse: 0.5384 - val_loss: 0.6405 - val_mse: 0.6405\n",
      "Epoch 25/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5428 - mse: 0.5428 - val_loss: 0.6268 - val_mse: 0.6268\n",
      "Epoch 26/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.5123 - mse: 0.5123 - val_loss: 0.6221 - val_mse: 0.6221\n",
      "Epoch 27/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5270 - mse: 0.5270 - val_loss: 0.6141 - val_mse: 0.6141\n",
      "Epoch 28/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4920 - mse: 0.4920 - val_loss: 0.5998 - val_mse: 0.5998\n",
      "Epoch 29/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5195 - mse: 0.5195 - val_loss: 0.5949 - val_mse: 0.5949\n",
      "Epoch 30/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4684 - mse: 0.4684 - val_loss: 0.5811 - val_mse: 0.5811\n",
      "Epoch 31/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4675 - mse: 0.4675 - val_loss: 0.5768 - val_mse: 0.5768\n",
      "Epoch 32/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4599 - mse: 0.4599 - val_loss: 0.5733 - val_mse: 0.5733\n",
      "Epoch 33/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4484 - mse: 0.4484 - val_loss: 0.5628 - val_mse: 0.5628\n",
      "Epoch 34/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4675 - mse: 0.4675 - val_loss: 0.5560 - val_mse: 0.5560\n",
      "Epoch 35/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4381 - mse: 0.4381 - val_loss: 0.6195 - val_mse: 0.6195\n",
      "Epoch 36/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4505 - mse: 0.4505 - val_loss: 0.5525 - val_mse: 0.5525\n",
      "Epoch 37/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4214 - mse: 0.4214 - val_loss: 0.5708 - val_mse: 0.5708\n",
      "Epoch 38/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.4228 - mse: 0.4228 - val_loss: 0.5393 - val_mse: 0.5393\n",
      "Epoch 39/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4177 - mse: 0.4177 - val_loss: 0.5481 - val_mse: 0.5481\n",
      "Epoch 40/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4042 - mse: 0.4042 - val_loss: 0.5803 - val_mse: 0.5803\n",
      "Epoch 41/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4225 - mse: 0.4225 - val_loss: 0.5502 - val_mse: 0.5502\n",
      "Epoch 42/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3843 - mse: 0.3843 - val_loss: 0.5303 - val_mse: 0.5303\n",
      "Epoch 43/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3878 - mse: 0.3878 - val_loss: 0.5205 - val_mse: 0.5205\n",
      "Epoch 44/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3738 - mse: 0.3738 - val_loss: 0.5430 - val_mse: 0.5430\n",
      "Epoch 45/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3954 - mse: 0.3954 - val_loss: 0.5653 - val_mse: 0.5653\n",
      "Epoch 46/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3868 - mse: 0.3868 - val_loss: 0.5226 - val_mse: 0.5226\n",
      "Epoch 47/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3765 - mse: 0.3765 - val_loss: 0.5282 - val_mse: 0.5282\n",
      "Epoch 48/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3550 - mse: 0.3550 - val_loss: 0.5447 - val_mse: 0.5447\n",
      "Epoch 49/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3702 - mse: 0.3702 - val_loss: 0.5420 - val_mse: 0.5420\n",
      "Epoch 50/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3747 - mse: 0.3747 - val_loss: 0.5141 - val_mse: 0.5141\n",
      "Epoch 51/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3382 - mse: 0.3382 - val_loss: 0.5091 - val_mse: 0.5091\n",
      "Epoch 52/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3598 - mse: 0.3598 - val_loss: 0.5042 - val_mse: 0.5042\n",
      "Epoch 53/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3375 - mse: 0.3375 - val_loss: 0.5059 - val_mse: 0.5059\n",
      "Epoch 54/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3524 - mse: 0.3524 - val_loss: 0.5124 - val_mse: 0.5124\n",
      "Epoch 55/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3358 - mse: 0.3358 - val_loss: 0.5131 - val_mse: 0.5131\n",
      "Epoch 56/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3133 - mse: 0.3133 - val_loss: 0.4915 - val_mse: 0.4915\n",
      "Epoch 57/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3531 - mse: 0.3531 - val_loss: 0.4955 - val_mse: 0.4955\n",
      "Epoch 58/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.3378 - mse: 0.3378 - val_loss: 0.5215 - val_mse: 0.5215\n",
      "Epoch 59/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3149 - mse: 0.3149 - val_loss: 0.5118 - val_mse: 0.5118\n",
      "Epoch 60/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3069 - mse: 0.3069 - val_loss: 0.5309 - val_mse: 0.5309\n",
      "Epoch 61/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3172 - mse: 0.3172 - val_loss: 0.5013 - val_mse: 0.5013\n",
      "Epoch 62/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3055 - mse: 0.3055 - val_loss: 0.4872 - val_mse: 0.4872\n",
      "Epoch 63/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3044 - mse: 0.3044 - val_loss: 0.4958 - val_mse: 0.4958\n",
      "Epoch 64/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3033 - mse: 0.3033 - val_loss: 0.4933 - val_mse: 0.4933\n",
      "Epoch 65/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2942 - mse: 0.2942 - val_loss: 0.5016 - val_mse: 0.5016\n",
      "Epoch 66/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3025 - mse: 0.3025 - val_loss: 0.4913 - val_mse: 0.4913\n",
      "Epoch 67/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2915 - mse: 0.2915 - val_loss: 0.4931 - val_mse: 0.4931\n",
      "Epoch 68/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2932 - mse: 0.2932 - val_loss: 0.4913 - val_mse: 0.4913\n",
      "Epoch 69/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3090 - mse: 0.3090 - val_loss: 0.4851 - val_mse: 0.4851\n",
      "Epoch 70/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2855 - mse: 0.2855 - val_loss: 0.4962 - val_mse: 0.4962\n",
      "Epoch 71/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2935 - mse: 0.2935 - val_loss: 0.4882 - val_mse: 0.4882\n",
      "Epoch 72/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2888 - mse: 0.2888 - val_loss: 0.5031 - val_mse: 0.5031\n",
      "Epoch 73/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2749 - mse: 0.2749 - val_loss: 0.5029 - val_mse: 0.5029\n",
      "Epoch 74/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2577 - mse: 0.2577 - val_loss: 0.5247 - val_mse: 0.5247\n",
      "Epoch 75/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2629 - mse: 0.2629 - val_loss: 0.5420 - val_mse: 0.5420\n",
      "Epoch 76/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2745 - mse: 0.2745 - val_loss: 0.4841 - val_mse: 0.4841\n",
      "Epoch 77/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2853 - mse: 0.2853 - val_loss: 0.4915 - val_mse: 0.4915\n",
      "Epoch 78/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2621 - mse: 0.2621 - val_loss: 0.5092 - val_mse: 0.5092\n",
      "Epoch 79/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2597 - mse: 0.2597 - val_loss: 0.4974 - val_mse: 0.4974\n",
      "Epoch 80/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2734 - mse: 0.2734 - val_loss: 0.4939 - val_mse: 0.4939\n",
      "Epoch 81/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2739 - mse: 0.2739 - val_loss: 0.4905 - val_mse: 0.4905\n",
      "Epoch 82/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2522 - mse: 0.2522 - val_loss: 0.4931 - val_mse: 0.4931\n",
      "Epoch 83/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2512 - mse: 0.2512 - val_loss: 0.4918 - val_mse: 0.4918\n",
      "Epoch 84/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2508 - mse: 0.2508 - val_loss: 0.4852 - val_mse: 0.4852\n",
      "Epoch 85/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2570 - mse: 0.2570 - val_loss: 0.4863 - val_mse: 0.4863\n",
      "Epoch 86/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2470 - mse: 0.2470 - val_loss: 0.5022 - val_mse: 0.5022\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Training R2: 0.7429908229857746, Test R2: 0.6223792939626158\n",
      "Training MSE: 0.29996697972733605, Test MSE: 0.49141560261018746\n",
      "Training MAE: 0.3993730310160761, Test MAE: 0.5019285750022129\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "z_scores = np.abs(zscore(y_train))\n",
    "mask = (z_scores < 3)\n",
    "X_train_clean = X_train[mask]\n",
    "y_train_clean = y_train[mask]\n",
    "\n",
    "print(f\"Removed {len(X_train) - len(X_train_clean)} outliers from training set.\")\n",
    "\n",
    "neurons1 = 256\n",
    "neurons2 = 128\n",
    "neurons3 = 64\n",
    "learning_rate = 0.0001\n",
    "dropout_rate = 0.0\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(neurons1, activation=\"relu\", input_shape=(X_train_clean.shape[1],)),\n",
    "    keras.layers.Dropout(dropout_rate),\n",
    "    keras.layers.Dense(neurons2, activation=\"relu\"),\n",
    "    keras.layers.Dropout(dropout_rate),\n",
    "    keras.layers.Dense(neurons3, activation=\"relu\"),\n",
    "    keras.layers.Dropout(dropout_rate),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss=\"mse\",\n",
    "    metrics=[\"mse\"]\n",
    ")\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_mse',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_clean, y_train_clean,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_pred_train = model.predict(X_train_clean)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "r2_train = r2_score(y_train_clean, y_pred_train)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "mse_train = mean_squared_error(y_train_clean, y_pred_train)\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "mae_train = mean_absolute_error(y_train_clean, y_pred_train)\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Training R2: {r2_train}, Test R2: {r2_test}\")\n",
    "print(f\"Training MSE: {mse_train}, Test MSE: {mse_test}\")\n",
    "print(f\"Training MAE: {mae_train}, Test MAE: {mae_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafc1271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R2: 0.7312894194131043, Test R2: 0.6207344283340006\n",
      "Training MSE: 0.30192795675673206, Test MSE: 0.43512580274836554\n",
      "Training MAE: 0.4014599406236926, Test MAE: 0.47722860130077155\n"
     ]
    }
   ],
   "source": [
    "# The following applicability domains use variables produced from the ANN code above.\n",
    "\n",
    "ad = TopKatApplicabilityDomain()\n",
    "ad.fit(X_train_clean)\n",
    "\n",
    "inside_ad_train = ad.contains(X_train_clean)\n",
    "inside_ad_test = ad.contains(X_test)\n",
    "\n",
    "y_train_inside = y_train_clean[inside_ad_train]\n",
    "y_pred_train_inside = y_pred_train[inside_ad_train]\n",
    "\n",
    "y_test_inside = y_test[inside_ad_test]\n",
    "y_pred_test_inside = y_pred_test[inside_ad_test]\n",
    "\n",
    "r2_train = r2_score(y_train_inside, y_pred_train_inside)\n",
    "r2_test = r2_score(y_test_inside, y_pred_test_inside)\n",
    "mse_train = mean_squared_error(y_train_inside, y_pred_train_inside)\n",
    "mse_test = mean_squared_error(y_test_inside, y_pred_test_inside)\n",
    "mae_train = mean_absolute_error(y_train_inside, y_pred_train_inside)\n",
    "mae_test = mean_absolute_error(y_test_inside, y_pred_test_inside)\n",
    "\n",
    "print(f\"Training R2: {r2_train}, Test R2: {r2_test}\")\n",
    "print(f\"Training MSE: {mse_train}, Test MSE: {mse_test}\")\n",
    "print(f\"Training MAE: {mae_train}, Test MAE: {mae_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc6f3903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R2: 0.744966535014033, Test R2: 0.6252611886164933\n",
      "Training MSE: 0.3029860494465483, Test MSE: 0.5034080156420836\n",
      "Training MAE: 0.40083283470695846, Test MAE: 0.5063258336105685\n"
     ]
    }
   ],
   "source": [
    "# The following applicability domains use variables produced from the ANN code above.\n",
    "\n",
    "ad = KNNApplicabilityDomain()\n",
    "ad.fit(X_train_clean)\n",
    "\n",
    "inside_ad_train = ad.contains(X_train_clean)\n",
    "inside_ad_test = ad.contains(X_test)\n",
    "\n",
    "y_train_inside = y_train_clean[inside_ad_train]\n",
    "y_pred_train_inside = y_pred_train[inside_ad_train]\n",
    "\n",
    "y_test_inside = y_test[inside_ad_test]\n",
    "y_pred_test_inside = y_pred_test[inside_ad_test]\n",
    "\n",
    "r2_train = r2_score(y_train_inside, y_pred_train_inside)\n",
    "r2_test = r2_score(y_test_inside, y_pred_test_inside)\n",
    "mse_train = mean_squared_error(y_train_inside, y_pred_train_inside)\n",
    "mse_test = mean_squared_error(y_test_inside, y_pred_test_inside)\n",
    "mae_train = mean_absolute_error(y_train_inside, y_pred_train_inside)\n",
    "mae_test = mean_absolute_error(y_test_inside, y_pred_test_inside)\n",
    "\n",
    "print(f\"Training R2: {r2_train}, Test R2: {r2_test}\")\n",
    "print(f\"Training MSE: {mse_train}, Test MSE: {mse_test}\")\n",
    "print(f\"Training MAE: {mae_train}, Test MAE: {mae_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9068fc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 93 outliers using IQR method.\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rafael\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 30.1157 - mse: 30.1157 - val_loss: 1.9512 - val_mse: 1.9512\n",
      "Epoch 2/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5382 - mse: 1.5382 - val_loss: 1.0824 - val_mse: 1.0824\n",
      "Epoch 3/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9591 - mse: 0.9591 - val_loss: 0.9907 - val_mse: 0.9907\n",
      "Epoch 4/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8624 - mse: 0.8624 - val_loss: 0.9386 - val_mse: 0.9386\n",
      "Epoch 5/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8434 - mse: 0.8434 - val_loss: 0.9124 - val_mse: 0.9124\n",
      "Epoch 6/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8155 - mse: 0.8155 - val_loss: 0.8911 - val_mse: 0.8911\n",
      "Epoch 7/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8154 - mse: 0.8154 - val_loss: 0.8826 - val_mse: 0.8826\n",
      "Epoch 8/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7860 - mse: 0.7860 - val_loss: 0.8557 - val_mse: 0.8557\n",
      "Epoch 9/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7217 - mse: 0.7217 - val_loss: 0.8411 - val_mse: 0.8411\n",
      "Epoch 10/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7270 - mse: 0.7270 - val_loss: 0.8208 - val_mse: 0.8208\n",
      "Epoch 11/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7329 - mse: 0.7329 - val_loss: 0.8066 - val_mse: 0.8066\n",
      "Epoch 12/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6917 - mse: 0.6917 - val_loss: 0.8015 - val_mse: 0.8015\n",
      "Epoch 13/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6953 - mse: 0.6953 - val_loss: 0.7934 - val_mse: 0.7934\n",
      "Epoch 14/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7070 - mse: 0.7070 - val_loss: 0.7798 - val_mse: 0.7798\n",
      "Epoch 15/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6545 - mse: 0.6545 - val_loss: 0.8084 - val_mse: 0.8084\n",
      "Epoch 16/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6999 - mse: 0.6999 - val_loss: 0.7543 - val_mse: 0.7543\n",
      "Epoch 17/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6206 - mse: 0.6206 - val_loss: 0.7256 - val_mse: 0.7256\n",
      "Epoch 18/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6382 - mse: 0.6382 - val_loss: 0.7110 - val_mse: 0.7110\n",
      "Epoch 19/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6081 - mse: 0.6081 - val_loss: 0.7067 - val_mse: 0.7067\n",
      "Epoch 20/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6220 - mse: 0.6220 - val_loss: 0.7026 - val_mse: 0.7026\n",
      "Epoch 21/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5713 - mse: 0.5713 - val_loss: 0.7108 - val_mse: 0.7108\n",
      "Epoch 22/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5615 - mse: 0.5615 - val_loss: 0.6672 - val_mse: 0.6672\n",
      "Epoch 23/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6069 - mse: 0.6069 - val_loss: 0.6583 - val_mse: 0.6583\n",
      "Epoch 24/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5529 - mse: 0.5529 - val_loss: 0.6453 - val_mse: 0.6453\n",
      "Epoch 25/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5494 - mse: 0.5494 - val_loss: 0.6424 - val_mse: 0.6424\n",
      "Epoch 26/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5330 - mse: 0.5330 - val_loss: 0.6313 - val_mse: 0.6313\n",
      "Epoch 27/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5200 - mse: 0.5200 - val_loss: 0.6183 - val_mse: 0.6183\n",
      "Epoch 28/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4981 - mse: 0.4981 - val_loss: 0.6157 - val_mse: 0.6157\n",
      "Epoch 29/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4904 - mse: 0.4904 - val_loss: 0.6027 - val_mse: 0.6027\n",
      "Epoch 30/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5249 - mse: 0.5249 - val_loss: 0.6089 - val_mse: 0.6089\n",
      "Epoch 31/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5021 - mse: 0.5021 - val_loss: 0.5874 - val_mse: 0.5874\n",
      "Epoch 32/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4685 - mse: 0.4685 - val_loss: 0.6346 - val_mse: 0.6346\n",
      "Epoch 33/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4702 - mse: 0.4702 - val_loss: 0.5867 - val_mse: 0.5867\n",
      "Epoch 34/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4501 - mse: 0.4501 - val_loss: 0.6671 - val_mse: 0.6671\n",
      "Epoch 35/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4878 - mse: 0.4878 - val_loss: 0.5831 - val_mse: 0.5831\n",
      "Epoch 36/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4775 - mse: 0.4775 - val_loss: 0.5649 - val_mse: 0.5649\n",
      "Epoch 37/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4347 - mse: 0.4347 - val_loss: 0.5876 - val_mse: 0.5876\n",
      "Epoch 38/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4356 - mse: 0.4356 - val_loss: 0.5629 - val_mse: 0.5629\n",
      "Epoch 39/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4272 - mse: 0.4272 - val_loss: 0.5534 - val_mse: 0.5534\n",
      "Epoch 40/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4472 - mse: 0.4472 - val_loss: 0.5552 - val_mse: 0.5552\n",
      "Epoch 41/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3875 - mse: 0.3875 - val_loss: 0.5383 - val_mse: 0.5383\n",
      "Epoch 42/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4117 - mse: 0.4117 - val_loss: 0.5450 - val_mse: 0.5450\n",
      "Epoch 43/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4158 - mse: 0.4158 - val_loss: 0.5387 - val_mse: 0.5387\n",
      "Epoch 44/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3827 - mse: 0.3827 - val_loss: 0.5698 - val_mse: 0.5698\n",
      "Epoch 45/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.4032 - mse: 0.4032 - val_loss: 0.5479 - val_mse: 0.5479\n",
      "Epoch 46/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3937 - mse: 0.3937 - val_loss: 0.5288 - val_mse: 0.5288\n",
      "Epoch 47/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3755 - mse: 0.3755 - val_loss: 0.5671 - val_mse: 0.5671\n",
      "Epoch 48/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3741 - mse: 0.3741 - val_loss: 0.5221 - val_mse: 0.5221\n",
      "Epoch 49/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3745 - mse: 0.3745 - val_loss: 0.5571 - val_mse: 0.5571\n",
      "Epoch 50/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3903 - mse: 0.3903 - val_loss: 0.5131 - val_mse: 0.5131\n",
      "Epoch 51/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3507 - mse: 0.3507 - val_loss: 0.5301 - val_mse: 0.5301\n",
      "Epoch 52/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3435 - mse: 0.3435 - val_loss: 0.5187 - val_mse: 0.5187\n",
      "Epoch 53/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3509 - mse: 0.3509 - val_loss: 0.5104 - val_mse: 0.5104\n",
      "Epoch 54/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3491 - mse: 0.3491 - val_loss: 0.5142 - val_mse: 0.5142\n",
      "Epoch 55/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3431 - mse: 0.3431 - val_loss: 0.5495 - val_mse: 0.5495\n",
      "Epoch 56/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3403 - mse: 0.3403 - val_loss: 0.5286 - val_mse: 0.5286\n",
      "Epoch 57/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3285 - mse: 0.3285 - val_loss: 0.5041 - val_mse: 0.5041\n",
      "Epoch 58/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3282 - mse: 0.3282 - val_loss: 0.4980 - val_mse: 0.4980\n",
      "Epoch 59/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3276 - mse: 0.3276 - val_loss: 0.5047 - val_mse: 0.5047\n",
      "Epoch 60/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3233 - mse: 0.3233 - val_loss: 0.5392 - val_mse: 0.5392\n",
      "Epoch 61/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3389 - mse: 0.3389 - val_loss: 0.6526 - val_mse: 0.6526\n",
      "Epoch 62/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3617 - mse: 0.3617 - val_loss: 0.5029 - val_mse: 0.5029\n",
      "Epoch 63/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3065 - mse: 0.3065 - val_loss: 0.5192 - val_mse: 0.5192\n",
      "Epoch 64/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3111 - mse: 0.3111 - val_loss: 0.5045 - val_mse: 0.5045\n",
      "Epoch 65/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2995 - mse: 0.2995 - val_loss: 0.5010 - val_mse: 0.5010\n",
      "Epoch 66/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2945 - mse: 0.2945 - val_loss: 0.4959 - val_mse: 0.4959\n",
      "Epoch 67/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2997 - mse: 0.2997 - val_loss: 0.4963 - val_mse: 0.4963\n",
      "Epoch 68/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3040 - mse: 0.3040 - val_loss: 0.5092 - val_mse: 0.5092\n",
      "Epoch 69/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3070 - mse: 0.3070 - val_loss: 0.5028 - val_mse: 0.5028\n",
      "Epoch 70/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.3016 - mse: 0.3016 - val_loss: 0.4974 - val_mse: 0.4974\n",
      "Epoch 71/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2822 - mse: 0.2822 - val_loss: 0.5101 - val_mse: 0.5101\n",
      "Epoch 72/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2892 - mse: 0.2892 - val_loss: 0.5519 - val_mse: 0.5519\n",
      "Epoch 73/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2978 - mse: 0.2978 - val_loss: 0.4915 - val_mse: 0.4915\n",
      "Epoch 74/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2740 - mse: 0.2740 - val_loss: 0.4999 - val_mse: 0.4999\n",
      "Epoch 75/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2814 - mse: 0.2814 - val_loss: 0.5084 - val_mse: 0.5084\n",
      "Epoch 76/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2893 - mse: 0.2893 - val_loss: 0.5073 - val_mse: 0.5073\n",
      "Epoch 77/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2708 - mse: 0.2708 - val_loss: 0.5121 - val_mse: 0.5121\n",
      "Epoch 78/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2829 - mse: 0.2829 - val_loss: 0.5453 - val_mse: 0.5453\n",
      "Epoch 79/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2814 - mse: 0.2814 - val_loss: 0.4928 - val_mse: 0.4928\n",
      "Epoch 80/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2538 - mse: 0.2538 - val_loss: 0.4971 - val_mse: 0.4971\n",
      "Epoch 81/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2806 - mse: 0.2806 - val_loss: 0.5062 - val_mse: 0.5062\n",
      "Epoch 82/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2661 - mse: 0.2661 - val_loss: 0.4915 - val_mse: 0.4915\n",
      "Epoch 83/100\n",
      "\u001b[1m150/150\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2717 - mse: 0.2717 - val_loss: 0.5138 - val_mse: 0.5138\n",
      "\u001b[1m187/187\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 965us/step\n",
      "Training R2: 0.7231149302281672, Test R2: 0.6184692795728146\n",
      "Training MSE: 0.31471140616115656, Test MSE: 0.49650388841353116\n",
      "Training MAE: 0.4151830011465461, Test MAE: 0.5143475764858243\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import pandas as pd\n",
    "\n",
    "y_train_series = pd.Series(y_train)\n",
    "\n",
    "Q1 = y_train_series.quantile(0.25)\n",
    "Q3 = y_train_series.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "mask = (y_train_series >= lower_bound) & (y_train_series <= upper_bound)\n",
    "X_train_clean = X_train[mask]\n",
    "y_train_clean = y_train[mask]\n",
    "\n",
    "print(f\"Removed {len(X_train) - len(X_train_clean)} outliers using IQR method.\")\n",
    "\n",
    "\n",
    "neurons1 = 256\n",
    "neurons2 = 128\n",
    "neurons3 = 64\n",
    "learning_rate = 0.0001\n",
    "dropout_rate = 0.0\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(neurons1, activation=\"relu\", input_shape=(X_train_clean.shape[1],)),\n",
    "    keras.layers.Dropout(dropout_rate),\n",
    "    keras.layers.Dense(neurons2, activation=\"relu\"),\n",
    "    keras.layers.Dropout(dropout_rate),\n",
    "    keras.layers.Dense(neurons3, activation=\"relu\"),\n",
    "    keras.layers.Dropout(dropout_rate),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss=\"mse\",\n",
    "    metrics=[\"mse\"]\n",
    ")\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_mse',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_clean, y_train_clean,\n",
    "    epochs=100,\n",
    "    validation_split=0.2,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_pred_train = model.predict(X_train_clean)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "r2_train = r2_score(y_train_clean, y_pred_train)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "mse_train = mean_squared_error(y_train_clean, y_pred_train)\n",
    "mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "mae_train = mean_absolute_error(y_train_clean, y_pred_train)\n",
    "mae_test = mean_absolute_error(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Training R2: {r2_train}, Test R2: {r2_test}\")\n",
    "print(f\"Training MSE: {mse_train}, Test MSE: {mse_test}\")\n",
    "print(f\"Training MAE: {mae_train}, Test MAE: {mae_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e0bba8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R2: 0.7134630679178048, Test R2: 0.61411902746764\n",
      "Training MSE: 0.3149858968824533, Test MSE: 0.4433140671241943\n",
      "Training MAE: 0.41724204220077343, Test MAE: 0.4925239230498098\n"
     ]
    }
   ],
   "source": [
    "# The following applicability domains use variables produced from the ANN code above.\n",
    "\n",
    "ad = TopKatApplicabilityDomain()\n",
    "ad.fit(X_train_clean)\n",
    "\n",
    "inside_ad_train = ad.contains(X_train_clean)\n",
    "inside_ad_test = ad.contains(X_test)\n",
    "\n",
    "y_train_inside = y_train_clean[inside_ad_train]\n",
    "y_pred_train_inside = y_pred_train[inside_ad_train]\n",
    "\n",
    "y_test_inside = y_test[inside_ad_test]\n",
    "y_pred_test_inside = y_pred_test[inside_ad_test]\n",
    "\n",
    "r2_train = r2_score(y_train_inside, y_pred_train_inside)\n",
    "r2_test = r2_score(y_test_inside, y_pred_test_inside)\n",
    "mse_train = mean_squared_error(y_train_inside, y_pred_train_inside)\n",
    "mse_test = mean_squared_error(y_test_inside, y_pred_test_inside)\n",
    "mae_train = mean_absolute_error(y_train_inside, y_pred_train_inside)\n",
    "mae_test = mean_absolute_error(y_test_inside, y_pred_test_inside)\n",
    "\n",
    "print(f\"Training R2: {r2_train}, Test R2: {r2_test}\")\n",
    "print(f\"Training MSE: {mse_train}, Test MSE: {mse_test}\")\n",
    "print(f\"Training MAE: {mae_train}, Test MAE: {mae_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be69c54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R2: 0.7716420827701251, Test R2: 0.6563263569442341\n",
      "Training MSE: 0.311985795988654, Test MSE: 0.4616764034674082\n",
      "Training MAE: 0.4047105621657717, Test MAE: 0.48835161900927826\n"
     ]
    }
   ],
   "source": [
    "# The following applicability domains use variables produced from the ANN code above.\n",
    "\n",
    "ad = KNNApplicabilityDomain()\n",
    "ad.fit(X_train)\n",
    "\n",
    "inside_ad_train = ad.contains(X_train)\n",
    "inside_ad_test = ad.contains(X_test)\n",
    "\n",
    "y_train_inside = y_train[inside_ad_train]\n",
    "y_pred_train_inside = y_pred_train[inside_ad_train]\n",
    "\n",
    "y_test_inside = y_test[inside_ad_test]\n",
    "y_pred_test_inside = y_pred_test[inside_ad_test]\n",
    "\n",
    "r2_train = r2_score(y_train_inside, y_pred_train_inside)\n",
    "r2_test = r2_score(y_test_inside, y_pred_test_inside)\n",
    "mse_train = mean_squared_error(y_train_inside, y_pred_train_inside)\n",
    "mse_test = mean_squared_error(y_test_inside, y_pred_test_inside)\n",
    "mae_train = mean_absolute_error(y_train_inside, y_pred_train_inside)\n",
    "mae_test = mean_absolute_error(y_test_inside, y_pred_test_inside)\n",
    "\n",
    "print(f\"Training R2: {r2_train}, Test R2: {r2_test}\")\n",
    "print(f\"Training MSE: {mse_train}, Test MSE: {mse_test}\")\n",
    "print(f\"Training MAE: {mae_train}, Test MAE: {mae_test}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
